name: Database Maintenance

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      action:
        description: 'Maintenance action to perform'
        required: true
        default: 'backup'
        type: choice
        options:
          - backup
          - vacuum
          - analyze
          - reindex

env:
  POSTGRES_VERSION: '15'

jobs:
  database-backup:
    name: Database Backup
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.action == 'backup'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup PostgreSQL client
      run: |
        sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
        wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
        sudo apt-get update
        sudo apt-get -y install postgresql-client-${{ env.POSTGRES_VERSION }}

    - name: Create backup
      env:
        DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
      run: |
        # Parse database URL
        DB_HOST=$(echo $DATABASE_URL | sed -n 's/.*@\([^:]*\):.*/\1/p')
        DB_PORT=$(echo $DATABASE_URL | sed -n 's/.*:\([0-9]*\)\/.*/\1/p')
        DB_NAME=$(echo $DATABASE_URL | sed -n 's/.*\/\([^?]*\).*/\1/p')
        DB_USER=$(echo $DATABASE_URL | sed -n 's/.*:\/\/\([^:]*\):.*/\1/p')
        DB_PASS=$(echo $DATABASE_URL | sed -n 's/.*:\/\/[^:]*:\([^@]*\)@.*/\1/p')
        
        # Create backup filename with timestamp
        BACKUP_FILE="scholarship_db_$(date +%Y%m%d_%H%M%S).sql"
        
        # Perform backup
        PGPASSWORD=$DB_PASS pg_dump \
          -h $DB_HOST \
          -p $DB_PORT \
          -U $DB_USER \
          -d $DB_NAME \
          -f $BACKUP_FILE \
          --verbose \
          --no-owner \
          --no-privileges \
          --if-exists \
          --clean
        
        # Compress backup
        gzip $BACKUP_FILE
        
        echo "backup_file=${BACKUP_FILE}.gz" >> $GITHUB_ENV
        echo "backup_size=$(ls -lh ${BACKUP_FILE}.gz | awk '{print $5}')" >> $GITHUB_ENV

    - name: Upload backup to S3
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: ${{ secrets.AWS_REGION }}
        S3_BUCKET: ${{ secrets.BACKUP_S3_BUCKET }}
      run: |
        # Install AWS CLI
        pip install awscli
        
        # Upload to S3 with lifecycle management
        aws s3 cp ${{ env.backup_file }} s3://${S3_BUCKET}/database-backups/
        
        # Keep only last 30 days of backups
        aws s3 ls s3://${S3_BUCKET}/database-backups/ | \
          awk '{print $4}' | \
          sort -r | \
          tail -n +31 | \
          xargs -I {} aws s3 rm s3://${S3_BUCKET}/database-backups/{}

    - name: Notify backup status
      if: always()
      run: |
        if [ "${{ job.status }}" == "success" ]; then
          echo "✅ Database backup completed successfully" >> $GITHUB_STEP_SUMMARY
          echo "- Backup file: ${{ env.backup_file }}" >> $GITHUB_STEP_SUMMARY
          echo "- Size: ${{ env.backup_size }}" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ Database backup failed!" >> $GITHUB_STEP_SUMMARY
        fi

  database-maintenance:
    name: Database Maintenance
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action != 'backup'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup PostgreSQL client
      run: |
        sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
        wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
        sudo apt-get update
        sudo apt-get -y install postgresql-client-${{ env.POSTGRES_VERSION }}

    - name: Run maintenance task
      env:
        DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
        MAINTENANCE_ACTION: ${{ github.event.inputs.action }}
      run: |
        # Parse database URL
        DB_HOST=$(echo $DATABASE_URL | sed -n 's/.*@\([^:]*\):.*/\1/p')
        DB_PORT=$(echo $DATABASE_URL | sed -n 's/.*:\([0-9]*\)\/.*/\1/p')
        DB_NAME=$(echo $DATABASE_URL | sed -n 's/.*\/\([^?]*\).*/\1/p')
        DB_USER=$(echo $DATABASE_URL | sed -n 's/.*:\/\/\([^:]*\):.*/\1/p')
        DB_PASS=$(echo $DATABASE_URL | sed -n 's/.*:\/\/[^:]*:\([^@]*\)@.*/\1/p')
        
        export PGPASSWORD=$DB_PASS
        
        case $MAINTENANCE_ACTION in
          vacuum)
            echo "Running VACUUM ANALYZE..."
            psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -c "VACUUM ANALYZE;"
            ;;
          analyze)
            echo "Running ANALYZE..."
            psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -c "ANALYZE;"
            ;;
          reindex)
            echo "Running REINDEX..."
            psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -c "REINDEX DATABASE $DB_NAME;"
            ;;
        esac
        
        # Get database statistics
        echo "### Database Statistics" >> $GITHUB_STEP_SUMMARY
        psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -c "
          SELECT 
            schemaname,
            tablename,
            pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
            n_live_tup AS rows
          FROM pg_stat_user_tables 
          ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
          LIMIT 10;
        " >> $GITHUB_STEP_SUMMARY

  cleanup-old-data:
    name: Cleanup Old Data
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run cleanup script
      working-directory: ./backend
      env:
        DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
      run: |
        # Create cleanup script
        cat > cleanup_old_data.py << 'EOF'
        import asyncio
        from datetime import datetime, timedelta
        from sqlalchemy import select, delete
        from app.db.session import async_session_maker
        from app.models.application import Application, ApplicationStatus
        from app.models.document import Document
        
        async def cleanup_old_data():
            async with async_session_maker() as session:
                # Delete draft applications older than 30 days
                cutoff_date = datetime.utcnow() - timedelta(days=30)
                
                result = await session.execute(
                    delete(Application)
                    .where(Application.status == ApplicationStatus.DRAFT)
                    .where(Application.updated_at < cutoff_date)
                )
                
                draft_count = result.rowcount
                
                # Delete orphaned documents
                orphaned_docs = await session.execute(
                    select(Document)
                    .outerjoin(Application)
                    .where(Application.id.is_(None))
                )
                
                orphan_count = len(orphaned_docs.scalars().all())
                
                await session.commit()
                
                print(f"Cleaned up {draft_count} old draft applications")
                print(f"Cleaned up {orphan_count} orphaned documents")
                
                return draft_count, orphan_count
        
        if __name__ == "__main__":
            draft_count, orphan_count = asyncio.run(cleanup_old_data())
        EOF
        
        python cleanup_old_data.py || echo "Cleanup script failed"

    - name: Generate cleanup report
      run: |
        echo "### Data Cleanup Report" >> $GITHUB_STEP_SUMMARY
        echo "- Date: $(date)" >> $GITHUB_STEP_SUMMARY
        echo "- Old draft applications removed" >> $GITHUB_STEP_SUMMARY
        echo "- Orphaned documents cleaned" >> $GITHUB_STEP_SUMMARY